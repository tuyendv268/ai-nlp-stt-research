{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "\n",
    "tqdm.pandas()\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_metadata(metadata_dirs):\n",
    "    metadata = []\n",
    "    for metadata_dir in metadata_dirs:\n",
    "        filepaths = glob(f'{metadata_dir}/*.jsonl')\n",
    "        for filepath in filepaths:\n",
    "            for line in open(filepath).readlines():\n",
    "                sample = json.loads(line)\n",
    "                metadata.append(sample)\n",
    "            \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data_dir = \"/data/asr-research/src/kaldi/data/f88_infer\"\n",
    "train_data_dir = \"/data/asr-research/src/kaldi/data/f88_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_data_dir):\n",
    "    os.mkdir(train_data_dir)\n",
    "    \n",
    "if not os.path.exists(infer_data_dir):\n",
    "    os.mkdir(infer_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3098.1582365829545)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dirs = [\n",
    "    \"/data/asr-research/data/s2_stt_metadata_w2v2_part_1\",\n",
    "    \"/data/asr-research/data/s2_stt_metadata_w2v2_part_2\"\n",
    "]\n",
    "metadata_v1 = load_cached_metadata(metadata_dirs)\n",
    "metadata_v1 = pd.DataFrame(metadata_v1)\n",
    "metadata_v1[\"id\"] = metadata_v1.audio_filepath.apply(lambda x: os.path.basename(x))\n",
    "metadata_v1 = metadata_v1.drop_duplicates(\"audio_filepath\")\n",
    "metadata_v1.duration.sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3128.4959655861726)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dirs = [\n",
    "    \"/data/asr-research/data/s2_stt_metadata_nemo_ctc\"\n",
    "]\n",
    "metadata_v2 = load_cached_metadata(metadata_dirs)\n",
    "metadata_v2 = pd.DataFrame(metadata_v2)\n",
    "metadata_v2[\"id\"] = metadata_v2.audio_filepath.apply(lambda x: os.path.basename(x))\n",
    "metadata_v2 = metadata_v2.drop_duplicates(\"audio_filepath\")\n",
    "metadata_v2 = metadata_v2.drop(columns=[\"text\"]).rename(columns={\"pred_text\": \"pred\"})\n",
    "metadata_v2.duration.sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2879186/2879186 [01:14<00:00, 38767.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2879186, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(metadata_v1, metadata_v2, on=\"id\", how=\"inner\", suffixes=[\"_nemo\", \"_w2v2\"])\n",
    "df[\"wer\"] = df.progress_apply(lambda x: wer(x[\"pred_w2v2\"], x[\"pred_nemo\"]), axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duration_w2v2.sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.wer <= 0.08].duration_w2v2.sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.wer < 0.05].duration_w2v2.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_same_segments(text_1, text_2):\n",
    "    sqm = difflib.SequenceMatcher(None, text_1, text_2)\n",
    "    result = sqm.get_matching_blocks()\n",
    "\n",
    "    matching_segments = [\n",
    "        \" \".join(text_1[segment.a: segment.a + segment.size])\n",
    "        for segment in result\n",
    "        if segment.size >= 3\n",
    "        ]\n",
    "    \n",
    "    return matching_segments\n",
    "\n",
    "def save(metadata, data_dir):\n",
    "    wavscp_path = f'{data_dir}/wav.scp'\n",
    "    text_path = f'{data_dir}/text'\n",
    "    spk2utt_path = f'{data_dir}/spk2utt'\n",
    "    utt2spk_path = f'{data_dir}/utt2spk'\n",
    "    meta_path = f'{data_dir}/meta.jsonl'\n",
    "\n",
    "    def create_text_file(f, contents):\n",
    "        line = \"\\t\".join(contents)\n",
    "        f.write(line + \"\\n\")\n",
    "        \n",
    "    with open(meta_path, \"w\") as f:\n",
    "        for index in metadata.index:\n",
    "            json_obj = json.dumps(metadata.loc[index].to_dict(), ensure_ascii=False)\n",
    "            f.write(json_obj + \"\\n\")\n",
    "        print(f'###saved to: {meta_path}')\n",
    "\n",
    "    with open(wavscp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        metadata.sort_values(\"id\").apply(lambda x: create_text_file(f, (x[\"id\"], x[\"audio_filepath_w2v2\"])), axis=1)\n",
    "        print(f'###saved to: {wavscp_path}')\n",
    "\n",
    "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        metadata.sort_values(\"id\").apply(lambda x: create_text_file(f, (x[\"id\"], x[\"pred_w2v2\"])), axis=1)\n",
    "        print(f'###saved to: {text_path}')\n",
    "        \n",
    "    with open(spk2utt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        metadata.sort_values(\"id\").apply(lambda x: create_text_file(f, (x[\"id\"], x[\"id\"])), axis=1)\n",
    "        print(f'###saved to: {spk2utt_path}')\n",
    "        \n",
    "    with open(utt2spk_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        metadata.sort_values(\"id\").apply(lambda x: create_text_file(f, (x[\"id\"], x[\"id\"])), axis=1)\n",
    "        print(f'###saved to: {utt2spk_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = df[df.wer > 0.05].copy()\n",
    "filtered_metadata[\"matching_segments\"] = filtered_metadata.parallel_apply(\n",
    "    lambda row: get_the_same_segments(\n",
    "        text_1=row[\"pred_w2v2\"].split(), \n",
    "        text_2=row[\"pred_nemo\"].split()\n",
    "        ), \n",
    "    axis=1\n",
    ")\n",
    "filtered_metadata.duration_w2v2.sum() / 3600, filtered_metadata.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(metadata=filtered_metadata, data_dir=infer_data_dir)\n",
    "# print(f'###saved infer data to {infer_data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = []\n",
    "for text, group in df[df.wer <= threshold].groupby(\"pred_w2v2\"):\n",
    "    if group.shape[0] > 32:\n",
    "        group = group.sample(32, random_state=42)\n",
    "\n",
    "    filtered_metadata.append(group)\n",
    "\n",
    "filtered_metadata = pd.concat(filtered_metadata)\n",
    "filtered_metadata.duration_w2v2.sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(metadata=filtered_metadata, data_dir=train_data_dir)\n",
    "# print(f'###saved train data to {train_data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = filtered_metadata[\n",
    "    [\n",
    "        \"audio_filepath_w2v2\", \"duration_w2v2\", \n",
    "        \"pred_w2v2\"\n",
    "        ]\n",
    "    ].copy()\n",
    "saved_df = saved_df.rename(\n",
    "    columns={\n",
    "        \"audio_filepath_w2v2\": \"audio_filepath\",\n",
    "        \"duration_w2v2\": \"duration\",\n",
    "        \"pred_w2v2\": \"text\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_metadata_filepath = f\"/data/asr-research/data/metadata/f88_wer_{threshold}_v1.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(clean_metadata_filepath, \"w\") as f:\n",
    "    for index in tqdm(saved_df.index):\n",
    "        row = saved_df.loc[index].to_dict()\n",
    "        json_obj = json.dumps(row, ensure_ascii=False)\n",
    "        f.write(json_obj + \"\\n\")\n",
    "\n",
    "print(f'###saved metadata to {clean_metadata_filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
